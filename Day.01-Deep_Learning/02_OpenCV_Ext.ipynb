{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGDgArzKUD1G"
   },
   "source": [
    "![Intel Logo](./images/logo.png)\n",
    "\n",
    "# Lecture 01-2. OpenCV Overview-2 (Filter, Feature Detection)\n",
    "\n",
    "### Contents :\n",
    "\n",
    "   1. Lecture 01 Overview <br>\n",
    "      1.1. Importing libraries and assigning variables\n",
    "   2. What are Kernels (2D filter)? <br>\n",
    "      2.1. Low-Pass Filters <br>\n",
    "      2.2. High-Pass Filters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.1. The Laplacian Filter <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.2. The Prewitt Filter <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.3. The Horizontal Prewitt Filter <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.4. The Vertical Prewitt Filter\n",
    "\n",
    "   3. Feature Detection <br>\n",
    "      3.1. What is a Feature? <br>\n",
    "      3.2. Feature Matching <br>\n",
    "      3.3. ORB\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2mtlLjStaWm"
   },
   "source": [
    "# 1. Lecture 01 Overview\n",
    "\n",
    "This lecture aims to explore Filters and Feature Detection using OpenCV.<br>\n",
    "The intended learning outcomes are:<br>\n",
    "*   Basic understanding and usage of Kernels and Filters\n",
    "*   Basic understanding of Feature Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfgaADsbUD1e"
   },
   "source": [
    "## 1.1 Importing libraries and assigning variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iURGrYR4UD1f"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# QUIZ: Assign \"bear.jpg\" to the img variable using the materials from the previous lecture, and resize the image to (512,512).\n",
    "img = cv2.imread(\"./images/bear.jpg\")\n",
    "img = cv2.resize(img,(512,512))\n",
    "\n",
    "# OpenCV uses BGR (Blue Green Red) while matplotlib uses RGB (Red Green Blue). Therefore, the channels will be back to front when the OpenCV image is loaded using matplotlib. Therefore, reverse the channel explicitly to maintain original color.\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# QUIZ: Convert the image to Grayscale and assign it to the img_gray variable.\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2RMw1MCUD1h"
   },
   "source": [
    "# 2. What are Kernels (2D filter)?\n",
    "\n",
    "The kernel, convolution matrix, or mask is a matrix that imparts effects such as blurring, sharpening, embossing, and edge detection to an image. This is done by performing convolution between the kernel and the image.\n",
    "\n",
    "Meanwhile, the term \"filter\" refers to a 3D structure in which several kernels are stacked. In the case of a 2D filter, the filter is the same as the kernel. In other words, the 2D filter, that is, the kernel, has its purpose to create an image that meets the user's purpose by changing the properties of the image through convolution.\n",
    "\n",
    "Filters are used very often in our daily life. For example, in the video conferencing platform such as Zoom, the user's background can be blurred, which is an example of using the Blur filter. We will practice applying various filters to the image below.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src='./images/bear.jpg' width=500 />\n",
    "</p>\n",
    "\n",
    "## 2.1 Low-Pass Filter\n",
    "\n",
    "The low-pass filter is also called a blurry filter or smoothing, and has the effect of blurring the image. Mathematically, the sum of all values in the filter is always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "vsDe8jkOUD1i",
    "outputId": "46c9db5b-88cb-4477-9361-9c491f72dd17"
   },
   "outputs": [],
   "source": [
    "# Blurry Filter, or Low Pass Filter \n",
    "# kernel = np.array([[1, 1, 1, 1, 1],\n",
    "#                   [1, 1, 1, 1, 1],\n",
    "#                   [1, 1, 1, 1, 1],\n",
    "#                   [1, 1, 1, 1, 1],\n",
    "#                   [1, 1, 1, 1, 1]])/25\n",
    "# kernel = np.ones((5,5))/25\n",
    "kernel = np.ones((10,10))/100\n",
    "\n",
    "# Convolving the kernels to the image\n",
    "dst = cv2.filter2D(img,-1,kernel) \n",
    "dst_gray = cv2.filter2D(img_gray,-1,kernel)\n",
    " \n",
    "# Visualizing the original and the blurred image\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Blurred Image')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the grayscale verions of the original and the blurred image\n",
    "plt.subplot(121),plt.imshow(img_gray,cmap=\"gray\"),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst_gray,cmap=\"gray\"),plt.title('Blurred Image')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR7_y4GdUD1l"
   },
   "source": [
    "## 2.2 High-Pass Filters\n",
    "\n",
    "Unlike low-pass filters, high-pass filters detect the edges in an image. It amplifies the slope of the edge, and makes the image clear as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cuCIM63UD1m"
   },
   "source": [
    "### 2.2.1 The Laplacian Filter\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src='./images/laplacian.jpg' />\n",
    "</p>\n",
    "\n",
    "Laplacian filter is an example of a high pass filter, and detects edges in all directions- horizontal and vertical. The Laplacian filter is a filter using the Laplacian function, and the function is visualized as shown in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uk-g-oUzUD1n",
    "outputId": "3c873c4b-e3ca-4e08-efb7-d7763f0a84df"
   },
   "outputs": [],
   "source": [
    "# Amongst the filters below, try to find the most suitable one for our specific case\n",
    "\n",
    "# 3x3 Negative Laplacian Operator - amplifies the inner edges of the image\n",
    "# kernel = np.array([ [0,-1,0],\n",
    "#                     [-1,4,-1],\n",
    "#                     [0,-1,0]])\n",
    "\n",
    "# 3x3 Positive Laplacian Operator - amplifies the outer edges of the image\n",
    "# kernel = np.array([ [0,1,0],\n",
    "#                     [1,-4,1],\n",
    "#                     [0,1,0]])\n",
    "\n",
    "# 3x3 Negative Discrete Laplacian Operator - the output is clearer than the kernels above\n",
    "# kernel = np.array([ [-1,-1,-1],\n",
    "#                     [-1,8,-1],\n",
    "#                     [-1,-1,-1]])\n",
    "\n",
    "# 3x3 Positive Discrete Laplacian Operator different version - the output is the clearest amongst the above kernels\n",
    "kernel = np.array([ [1,1,1],\n",
    "                    [1,-8,1],\n",
    "                    [1,1,1]])\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "# Visualizing the original and the filtered image\n",
    "dst = cv2.filter2D(img,-1,kernel)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Laplacian Filter')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the grayscale versions of the original and the filtered image\n",
    "dst_gray = cv2.filter2D(img_gray,-1,kernel)\n",
    "plt.subplot(121),plt.imshow(img_gray,cmap=\"gray\"),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst_gray,cmap=\"gray\"),plt.title('Laplacian Filter')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9lMl7DRUD1p"
   },
   "source": [
    "### 2.2.2 The Prewitt Filter\n",
    "\n",
    "If the Laplacian Filter is a filter that detects all edges regardless of direction, the Prewitt filter is a filter which detects edges in either vertical and horizontal directions separately. Refer to the Horizontal and Vertical Prewitt Filters below.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src='./images/prewitt.jpg' />  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmdOmM5iUD1q"
   },
   "source": [
    "### 2.2.3 The Horizontal Prewitt Filter\n",
    "\n",
    "The Horizontal Prewitt Filter detects and displays vertical edges on the output image using Vertical Prewitt Operator(Gx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZWDkB1PUD1r",
    "outputId": "4c044e91-51bd-4322-ec09-097a7c9656ac"
   },
   "outputs": [],
   "source": [
    "# Horizontal Filter (Gx)\n",
    "kernel = np.array([ [-1,0,1],\n",
    "                    [-1,0,1],\n",
    "                    [-1,0,1]])\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "# Visualizing the original and the filtered image\n",
    "dst = cv2.filter2D(img,-1,kernel)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Horizontal Sobel Filter')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the grayscale versions of the original and the filtered image\n",
    "dst_gray = cv2.filter2D(img_gray,-1,kernel)\n",
    "plt.subplot(121),plt.imshow(img_gray,cmap=\"gray\"),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst_gray,cmap=\"gray\"),plt.title('Horizontal Sobel Filter')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh_MxqiPUD1t"
   },
   "source": [
    "### 2.2.4 The Vertical Prewitt Filter\n",
    "\n",
    "Similarly, the Vertical Prewitt Filter detects and displays horizontal edges on the output image using Horizontal Prewitt Operator(Gy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjG4lKzQUD1u",
    "outputId": "e9ff348d-0a99-44dd-f159-d826eeb4d4b5"
   },
   "outputs": [],
   "source": [
    "# Vertical Filter (Gy)\n",
    "kernel = np.array([ [1,1,1],\n",
    "                    [0,0,0],\n",
    "                    [-1,-1,-1]])\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "# Visualizing the original and the filtered image\n",
    "dst = cv2.filter2D(img,-1,kernel)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Vertical Sobel Filter')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the grayscale versions of the original and the filtered image\n",
    "dst_gray = cv2.filter2D(img_gray,-1,kernel)\n",
    "plt.subplot(121),plt.imshow(img_gray,cmap=\"gray\"),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst_gray,cmap=\"gray\"),plt.title('Vertical Sobel Filter')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4fl49GpUD1v"
   },
   "source": [
    "So far, we have created a new image by applying filters to an image using OpenCV.\n",
    "\n",
    "In fact, we can use OpenCV to perform various tasks beyond image filtering, one of which is feature detection. In the following training sessions, let's explore how feature detection can be performed with only the OpenCV library before introducing deep learning.\n",
    "\n",
    "# 3. Feature Detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnvnN5XWUD1x"
   },
   "source": [
    "## 3.1 What is a Feature?\n",
    "\n",
    "Feature is information that is characteristic to the image such as the corners or vertices of the image. The unique features of the image are called Feature Points or Interest Points, which are points in the image that can be stably extracted in the process of different image variations such as changes in brightness, rotation, and magnification.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src='./images/featuredetection.jpg' />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHcJRoxtxOpw"
   },
   "source": [
    "## 3.2 Feature Matching\n",
    "\n",
    "Feature Matching is the process of establishing a correspondence relationship between two images of the same object. The same object can be matched with the same features between two images regardless of changes in brightness, rotation, or magnification.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src='./images/featuremapping.jpg' />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSM2byPgUD2F"
   },
   "source": [
    "## 3.3 ORB\n",
    "\n",
    "Amongst the algorithms that allow Feature Detection are SIFT, SURF, and ORB. We will look at ORB specifically, because it works best in various directions and magnifications.\n",
    "\n",
    "The ORB operation is executed as follows- first, after comparing the intensity of each pixel of the image and the surrounding pixels, if it exceeds a certain limit, the corresponding pixel is designated as a keypoint. When a keypoint is detected, a unique feature is created for each keypoint, and the features are matched between different images of the same object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8orxf7toUD2F",
    "outputId": "198d0e6f-061f-44e6-fc01-06e5530e309b"
   },
   "outputs": [],
   "source": [
    "# Gaussian Pyramid to change the image scale\n",
    "test_image = cv2.pyrDown(img)\n",
    "# test_image = cv2.pyrDown(test_image)\n",
    "\n",
    "# Rotating the image\n",
    "num_rows, num_cols = test_image.shape[:2]\n",
    "rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), 30, 1)\n",
    "test_image = cv2.warpAffine(test_image, rotation_matrix, (num_cols, num_rows))\n",
    "\n",
    "# Visualizing the image\n",
    "fx, plots = plt.subplots(1, 2, figsize=(20,10))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Training Image')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(test_image),plt.title('Testing Image')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwJaklj_UD2G",
    "outputId": "d079db66-cd40-4726-8744-9fed67f01dad"
   },
   "outputs": [],
   "source": [
    "# Executing the OpenCV ORB algorithm to extract Keypoints\n",
    "orb = cv2.ORB_create()\n",
    "train_keypoints, train_descriptor = orb.detectAndCompute(img, None)\n",
    "test_keypoints, test_descriptor = orb.detectAndCompute(test_image, None)\n",
    "keypoints_train = np.copy(img)\n",
    "keypoints_test = np.copy(test_image)\n",
    "cv2.drawKeypoints(img, train_keypoints, keypoints_train, color = (0, 255, 0))\n",
    "cv2.drawKeypoints(test_image, test_keypoints, keypoints_test, color = (0, 255, 0))\n",
    "\n",
    "# Visualizing Keypoints\n",
    "fx, plots = plt.subplots(1, 2, figsize=(20,10))\n",
    "plt.subplot(121),plt.imshow(keypoints_train),plt.title('Train Keypoints')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(keypoints_test),plt.title('Test Keypoints')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gk_6Ix5RUD2G",
    "outputId": "98b1cd4d-52b9-4c24-9224-c4523e9134ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Brute Force Matcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "\n",
    "# Feature matching between the ORB descriptor of the training image and the test image\n",
    "matches = bf.match(train_descriptor, test_descriptor)\n",
    "result = cv2.drawMatches(img, train_keypoints, test_image, test_keypoints, matches, None, flags=2)\n",
    "\n",
    "# Visualizing the matching points\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day1_L01_2-OpenCV_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
