{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSNiaP4P57qm"
   },
   "source": [
    "![Intel Logo](./images/logo.png)\n",
    "\n",
    "# Lecture 02. ANN Overview\n",
    "\n",
    "### Contents :\n",
    "   1. Lecture 02 Overview <br>\n",
    "      1.1. Importing libraries <br>\n",
    "   2. Keras <br>\n",
    "   3. The Iris Dataset <br>\n",
    "   4. Preparing, Compiling and Training the Model <br>\n",
    "   5. Validating and Testing the Model: Checking for Overfitting<br>\n",
    "   6. Dealing with Overfitting: Dropout, Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjKLbyFZ57qx"
   },
   "source": [
    "# 1. Lecture 02 Overview\n",
    "\n",
    "This lecture aims to explore preparing, compiling, training and improving the model using Keras.<br>\n",
    "The intended learning outcomes are:<br>\n",
    "\n",
    "*   Understanding and applying the Keras API\n",
    "*   Understanding overfitting and methods to overcome it\n",
    "## 1.1 Importing libraries\n",
    "\n",
    "First, import the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZT8j7Cpd57qy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from skimage import io\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjfvmKOfwcK_"
   },
   "source": [
    "# 2. Keras\n",
    "\n",
    "Keras is an Artificial Neural Network (ANN) library written in Python. Keras' intuitive API allows you to create various deep learning models with just a few lines of code. Using Keras, a model can be created easily by combining independent modules such as: neural network layer, loss function, activation function, and optimization algorithm modules, which are important concepts in deep learning.\n",
    "\n",
    "Let's apply what we have learned so far using the Keras API and The Iris Dataset. In order to perform the previously learnt classification task, we will use the small but reliable Iris Dataset which is widely used in classification examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhSjv-Yg57q0"
   },
   "source": [
    "# 3. The Iris Dataset\n",
    "\n",
    "Iris flowers are divided into several varieties according to the shape or length of the calyx or petal. In The Iris Dataset, Iris flowers are divided into three types according to the widths and lengths of the calyces and the petals.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src='./images/iris.png' width=700/>\n",
    "</p>\n",
    "\n",
    "After executing the cell, the figure displayed shows the classification of flowers according to each attribute (seed width/length, petal width/length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8L4cl70l57q2",
    "outputId": "eb7790bb-b177-41e0-997a-0f4eedead78b"
   },
   "outputs": [],
   "source": [
    "## Loading the Iris Dataset\n",
    "df = pd.read_csv('./iris.csv', names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"])\n",
    "\n",
    "## Checking the dataframe \n",
    "print(df.head(10))\n",
    "\n",
    "## Iris Data visualization (identifying attribute-variety distribution)\n",
    "sns.pairplot(df, hue='species');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t84ph9Gs57q4"
   },
   "source": [
    "# 4. Preparing, Compiling and Training the Model  \n",
    "\n",
    "We now want to design a model that will properly distinguish flowers and evaluate its accuracy. Since it is a classification task of choosing one of the three varieties, we call this multi-classification and set the node of the output layer to 3.\n",
    "\n",
    "The ratio of the training set and the testing set is set to 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-aJGMlXR57q5",
    "outputId": "aedc144e-7aca-4e92-d870-9bf3efc5672c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7f4cf0d0d0c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Converting the data into numeric values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchar_to_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "## Converting the data into numeric values\n",
    "dataset = df.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "char_to_num = LabelEncoder()\n",
    "char_to_num.fit(Y)\n",
    "Y = char_to_num.transform(Y)\n",
    "Y = np_utils.to_categorical(Y)\n",
    "\n",
    "## Splitting the dataset into training(80%) and testing(20%) sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "  X,\n",
    "  Y,\n",
    "  test_size=0.20\n",
    ")\n",
    "\n",
    "## Building the model\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(4,), activation=\"relu\"),\n",
    "    ##  QUIZ: Set the number of hidden layer nodes to 128 and use the relu activation function\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    ##  QUIZ: Set the number of output layer nodes to 3, use the softmax activation function and ____Fill in the blank____\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "## Compiling the model- categorical crossentropy is an error function suitable for multi classification.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "## Training the model\n",
    "history = model.fit(X_train, Y_train, epochs=200, validation_split=0.25, batch_size=10, verbose=2)\n",
    "\n",
    "## Print the output\n",
    "# print(\"\\n Accuracy: %.5f\" % (model.evaluate(X_train, Y_train)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alJlGQDq57q7"
   },
   "source": [
    "# 5. Validating and Testing the Model: Checking for Overfitting\n",
    "\n",
    "Overfitting refers to the phenomenon in which the model shows high accuracy using the training data, does not perform well with new data. The overfitting problem appears when there are too many layers or variables, or when there are too many complexities. Refer to the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4u72xUi57q8",
    "outputId": "74a908e7-f6f0-4450-b125-53971d774e46"
   },
   "outputs": [],
   "source": [
    "## Checking for overfitting\n",
    "train_metrics = history.history['loss']\n",
    "val_metrics = history.history['val_loss']\n",
    "epochs = range(1, len(train_metrics) + 1)\n",
    "plt.plot(epochs, train_metrics)\n",
    "plt.plot(epochs, val_metrics)\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkbiSnqe57q-"
   },
   "source": [
    "Referncing the figure above, it can be seen that the validation loss is higher than the training loss, and as the training loss decreases, the validation loss increases, which is a singal for overfitting.\n",
    "\n",
    "# 6. Dealing with Overfitting: Dropout, Batch Normalization\n",
    "\n",
    "We will apply Dropout and Batch normalization to overcome overfitting. Dropout is the act of dropping some of the nodes of the hidden layer. For example, Dropout(0.1) will drop 10% of the nodes in the respective layer. On the other hand, batch normalization refers to the activation value of the activation function being normalized and distributed appropriately. Although the mean and variance values of the data may be different, the gradient vanishing/exploding problem can be solved to some extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8i_fbJN57q_",
    "outputId": "ac993e26-eaec-4e4d-b8eb-6522d4423e01"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(4,), activation=\"relu\"),\n",
    "    ##  QUIZ: Drop 50% of the nodes in the respective layers\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "## Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "## Training the model\n",
    "history = model.fit(X_train, Y_train, epochs=200, validation_split=0.25, batch_size=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXaDl9cu57rB",
    "outputId": "bab0bcd0-04d5-4977-b7cd-5a9f68595bca"
   },
   "outputs": [],
   "source": [
    "train_metrics = history.history['loss']\n",
    "val_metrics = history.history['val_loss']\n",
    "epochs = range(1, len(train_metrics) + 1)\n",
    "plt.plot(epochs, train_metrics)\n",
    "plt.plot(epochs, val_metrics)\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHRUKxZe57rC"
   },
   "source": [
    "Compared with the figure in section 1.4, the training loss and the validation loss are similar and decreasing in both cases- indicating that overfitting can be solved to some extent through dropout.\n",
    "\n",
    "Next, we want to overcome overfitting through Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYtmiZDR57rD",
    "outputId": "a463f23e-7ae7-43e0-b716-4c8c84f4386b"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(4,), activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(3, activation='softmax')\n",
    "]);\n",
    "\n",
    "model.summary()\n",
    "\n",
    "## Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "## Training the model\n",
    "history = model.fit(X_train, Y_train, epochs=200, validation_split=0.25, batch_size=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uD0Ot5D57rF",
    "outputId": "87dd4ba0-048a-4662-a490-6d970c7d94e9"
   },
   "outputs": [],
   "source": [
    "train_metrics = history.history['loss']\n",
    "val_metrics = history.history['val_loss']\n",
    "epochs = range(1, len(train_metrics) + 1)\n",
    "plt.plot(epochs, train_metrics)\n",
    "plt.plot(epochs, val_metrics)\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o7TqwRf57rH"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day1_L02 - ANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
